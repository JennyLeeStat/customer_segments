{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning\n",
    "- Unsupervised learning finds patterns in the data\n",
    "- eg. clustering customers by their purchases\n",
    "- Compressing the data using purchase patterns (dimension reduction)\n",
    "\n",
    "## Supervised learning vs Unsupervised learning\n",
    "- supervised learning finds patterns for a prediction task\n",
    "- eg. classify tumors as benign or cancerous (labels)\n",
    "- Unsupervised learning finds patterns in the data unguided by labels, without a specific prediction task in mind\n",
    "\n",
    "\n",
    "# I. K-means clustering\n",
    "### 1.  Iris dataset\n",
    "- Iris samples are points in 4 dimensional space\n",
    "- dimension (number of features) too high to visualize \n",
    "- but unsupervised learning gives insight\n",
    "\n",
    "### 2. Fitting KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit()\n",
    "KMeans(algorithm='auto')\n",
    "labels = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluating a clustering\n",
    "- K-means found 3 clusters amongst the iris samples\n",
    "- Do the clusters correspond to the species?\n",
    "- you can use cross tabulation with pandas \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'labels': labels, 'species': species})\n",
    "ct = pd.crosstabl(df['labels'], df['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inertia measures clustering quality by measuring how spread out the clusters are\n",
    "- lower is better\n",
    "- distance from each sample to cetroid of its cluster\n",
    "- after fit(), available as attribute inertia_\n",
    "- in fact, k-means attempts to minimize the inertia when choosing clusters\n",
    "- inertia decreases as the number of clusters incereases\n",
    "- How to choose the number of clusters? \n",
    "- ultimately, is a trade off. A good clustering has tight clusters (so low inertia) but not too many clusters\n",
    "- good rule of thumb is choose an \"elbow\" (where inertia begins to decrease more slowly) in the inertia plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transforming features for better clusterings\n",
    "- often datasets has features with different variances\n",
    "- in kmeans, feature variance = feature influence\n",
    "- utilize `StandardScaler` to transform each feature to have mean 0 and variance 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.proprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "kmeans = KMeans(n_cluster=3)\n",
    "pipeline = make_pipeline(scaler, kmeans)\n",
    "pipeline.fit(samples)\n",
    "labels = pipeline.predict(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Visualizing hierarchies\n",
    "- t-SNE: creates a 2D map of a dataset\n",
    "- Hierarchical clustering\n",
    "\n",
    "## Hierarchical clustering\n",
    "\n",
    "### 1. Dataset - Eurovision song contest 2016\n",
    "### 2. Dendrogram\n",
    "- read from the bottom up\n",
    "- vertical lines represent clusters\n",
    "\n",
    "### 3. Agglomerative hierarchical clustering\n",
    "- every country begins in a separate cluster\n",
    "- at each step, the two closest clusters are merged\n",
    "- continue until all countries in a single cluster\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "mergings = linkage(samples, method='complete')\n",
    "dendrogram(mergings,\n",
    "          labels = country_names,\n",
    "          leaf_rotation=90,\n",
    "          leaf_font_size=6)\n",
    "plt.show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cluster labels\n",
    "- cluster labels at any intermediate stage can be recovered\n",
    "- for use in e.g. cross-tabulations\n",
    "- height on dendrogram = maximum distance between merging clusters, defined by linkage method\n",
    "- specified via method parameter\n",
    "- in \"complete\" linkage: distance between clusters is max. distance between their samples\n",
    "- In single linkage, the distance between clusters is the distance between the closest points of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "mergings = linkage(samples, method='complete')\n",
    "labels = fcluster(mergings, 15, criterion='distance')\n",
    "pairs = pd.DataFrame({'labels': labels,\n",
    "                     'countries': country_names})\n",
    "print(pairs.sort_values('labels'))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "- t-distributed stochastic neighbor embedding\n",
    "- maps samples to 2D space (or 3D)\n",
    "- map approximately preserves nearness of samples\n",
    "- great for inspecting dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(learning_rate=100)\n",
    "transformed = model.fit_transform(samples)\n",
    "xs = transformed[:, 0]\n",
    "ys = transformed[:, 1]\n",
    "plt.scatter(xs, ys, c=species)\n",
    "plt.show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### has only f`it_transform() `method\n",
    "- simultaneously fits the model and transforms the data\n",
    "- has no separate `fit()` or `transform()` methods\n",
    "- can't extend the map to include new data samples\n",
    "- must start over each time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learning rate\n",
    "- try many learning rate for different datasets\n",
    "- wrong choice: points bunch together\n",
    "- try values between 50 - 200\n",
    "\n",
    "\n",
    "#### different every time\n",
    "- t-SNE features are different every time\n",
    "- but have the same relevant position every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Create a TSNE instance: model\n",
    "model = TSNE(learning_rate=50)\n",
    "\n",
    "# Apply fit_transform to normalized_movements: tsne_features\n",
    "tsne_features = model.fit_transform(normalized_movements)\n",
    "\n",
    "# Select the 0th feature: xs\n",
    "xs = tsne_features[:, 0]\n",
    "\n",
    "# Select the 1th feature: ys\n",
    "ys = tsne_features[:,1]\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(xs, ys, alpha=0.5)\n",
    "\n",
    "# Annotate the points\n",
    "for x, y, company in zip(xs, ys, companies):\n",
    "    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Decorrelating your data \n",
    "Dimension reduction summarizes a dataset using its common occuring patterns. In this chapter, you'll learn about the most fundamental of dimension reduction techniques, \"Principal Component Analysis\" (\"PCA\"). PCA is often used before supervised learning to improve model performance and generalization. It can also be useful for unsupervised learning. For example, you'll employ a variant of PCA will allow you to cluster Wikipedia articles by their content!\n",
    "\n",
    "## 1. Dimension reduction\n",
    "- find the patterns in the data and use these patterns to re-express it in condensed form\n",
    "- more efficient storage and computation\n",
    "- remove less-informative \"noise\" features, which cause problem for subsequent prediction tasks\n",
    "\n",
    "\n",
    "## 2. PCA\n",
    "- PCA = principal component analysis\n",
    "- fundamental dimension reduction technique\n",
    "- first step: decorrelation\n",
    "- second step: reduces dimension\n",
    "\n",
    "\n",
    "## 3. First step: decorrelation\n",
    "- PCA aligns data with axes\n",
    "- rotate data samples to be aligned with axes\n",
    "- shifts data samples so they have mean 0\n",
    "- no information is lost\n",
    "- retain the same number of rows and columns\n",
    "- rows of transformed corresponds to samples\n",
    "- columns of transformed are the PCA features\n",
    "- transformed PCA features are not linearly correlated ('decorrelation')\n",
    "\n",
    "\n",
    "## 4. Principal components\n",
    "- principal components = directions of variance\n",
    "- avaiable as components_ attribute of PCA objects\n",
    "- each row defines displacement from mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Intrinsic dimension\n",
    "- intrinsic dimension: number of features needed to approximate the dataset\n",
    "- essential idea behind dimension reduction\n",
    "- what is the most compact representatino of the samples?\n",
    "- can be detected with PCA \n",
    "- scatter plots works only if samples have 2 or 3 features\n",
    "- PCA identifies intrinsic dimension when samples have any number of features\n",
    "- intrinsic dimension: number of PCA features with significant variance\n",
    "- an idealization, in real life there is not alywas one correct answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create scaler: scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create a PCA instance: pca\n",
    "pca = PCA()\n",
    "\n",
    "# Create pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "\n",
    "# Fit the pipeline to 'samples'\n",
    "pipeline.fit(samples)\n",
    "\n",
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xlabel('PCA feature')\n",
    "plt.ylabel('variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## 6. Second step: dimension reduction\n",
    "- represents same data, using less features\n",
    "- important part of machine-learning pipelines\n",
    "- can be performed using PCA\n",
    "- PCA features are in decreasing order of variance\n",
    "- assumes the low variance features are \"noise\"\n",
    "- the high variance features are informative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- specify how many features to keep\n",
    "- eg. PCA(n_components=2)\n",
    "- keeps the first 2 PCA features\n",
    "- intrinsic dimension is a good choice\n",
    "- discard low variance PCA features\n",
    "- assumption typically holds in practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7. Alternative implication of PCA\n",
    "### word frequency arrays - rows: documents, columns - words\n",
    "- entries measure presence of each workd in each documents\n",
    "- measure using tf-idf\n",
    "- tf: frequency of word in document\n",
    "- idf: reduces influence of frequent words such as \"the\"\n",
    "- this array is \"sparse\", most entries are zero\n",
    "- can use scipy.sparse.csr_matrix instead of NumPy array\n",
    "- sklearn PCA doesn't support crs_matrix, use `TruncatedSVD` instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer: tfidf\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Apply fit_transform to document: csr_mat\n",
    "csr_mat = tfidf.fit_transform(documents)\n",
    "\n",
    "# Print result of toarray() method\n",
    "print(csr_mat.toarray())\n",
    "\n",
    "# Get the words: words\n",
    "words = tfidf.get_feature_names()\n",
    "\n",
    "# Print words\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a TruncatedSVD instance: svd\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "\n",
    "# Create a KMeans instance: kmeans\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "\n",
    "# Create a pipeline: pipeline\n",
    "pipeline = make_pipeline(svd, kmeans)\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Fit the pipeline to articles\n",
    "pipeline.fit(articles)\n",
    "\n",
    "# Calculate the cluster labels: labels\n",
    "labels = pipeline.predict(articles)\n",
    "\n",
    "# Create a DataFrame aligning labels and titles: df\n",
    "df = pd.DataFrame({'label': labels, 'article': titles})\n",
    "\n",
    "# Display df sorted by cluster label\n",
    "print(df.sort_values(by='label'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# IV. NMF\n",
    "- non-negative matrix factorization\n",
    "- dimension reduction technique\n",
    "- unlike PCA, NMF models are interpretable\n",
    "- all sample features must be non-negative\n",
    "- NMF expresses documnets as combinations of topics or themes\n",
    "- NMF expresses images as combinations of patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. schikit learn NMF\n",
    "- follows fit/transform pattern\n",
    "- must specify number of components\n",
    "- works with Numpy arrays and with csr_matrix\n",
    "\n",
    "\n",
    "### 2. NMF components\n",
    "- just like PCA NMF has components\n",
    "- dimension of components = dimension of samples\n",
    "- entries are non-negative\n",
    "\n",
    "\n",
    "### 3. NMF features\n",
    "- NMF feature values are non-negative\n",
    "- can be used to reconstruct the samples\n",
    "- combine feature values with components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import NMF\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create an NMF instance: model\n",
    "model = NMF(n_components=6)\n",
    "\n",
    "# Fit the model to articles\n",
    "model.fit(articles)\n",
    "\n",
    "# Transform the articles: nmf_features\n",
    "nmf_features = model.transform(articles)\n",
    "\n",
    "# Print the NMF features\n",
    "print(nmf_features)\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a pandas DataFrame: df\n",
    "df = pd.DataFrame(nmf_features, index=titles)\n",
    "\n",
    "# Print the row for 'Anne Hathaway'\n",
    "print(df.loc['Anne Hathaway', :])\n",
    "\n",
    "# Print the row for 'Denzel Washington'\n",
    "print(df.loc['Denzel Washington', :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. sample reconstruction\n",
    "- multiply components by feature values, and add up\n",
    "- can also be expressed as a product of matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. NMF learns interpretable parts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 6. Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalize the NMF features: norm_features\n",
    "norm_features = normalize(nmf_features)\n",
    "\n",
    "# Create a DataFrame: df\n",
    "df = pd.DataFrame(norm_features, index=titles)\n",
    "\n",
    "# Select the row corresponding to 'Cristiano Ronaldo': article\n",
    "article = df.loc['Cristiano Ronaldo', :]\n",
    "\n",
    "# Compute the dot products: similarities\n",
    "similarities = df.dot(article)\n",
    "\n",
    "# Display those with the largest cosine similarity\n",
    "print(similarities.nlargest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform the necessary imports\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a MaxAbsScaler: scaler\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "# Create an NMF model: nmf\n",
    "nmf = NMF(n_components=20)\n",
    "\n",
    "# Create a Normalizer: normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Create a pipeline: pipeline\n",
    "pipeline = make_pipeline(scaler, nmf, normalizer)\n",
    "\n",
    "# Apply fit_transform to artists: norm_features\n",
    "norm_features = pipeline.fit_transform(artists)\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame: df\n",
    "df = pd.DataFrame(norm_features, index=artist_names)\n",
    "\n",
    "# Select row of 'Bruce Springsteen': artist\n",
    "artist = df.loc['Bruce Springsteen', :]\n",
    "\n",
    "# Compute cosine similarities: similarities\n",
    "similarities = df.dot(artist)\n",
    "\n",
    "# Display those with highest cosine similarity\n",
    "print(similarities.nlargest())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
